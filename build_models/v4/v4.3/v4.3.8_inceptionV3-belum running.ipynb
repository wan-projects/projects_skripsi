{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c03f9bba",
   "metadata": {},
   "source": [
    "# Note\n",
    "1. Melakukan percobaan build_models v4.0\n",
    "2. Dilakukan pada hari Sabtu, 20 Juli 2024\n",
    "3. Tempat di LAB\n",
    "4. Data menggunakan Dataset yang dari Roboflow \n",
    "4. Meliputi Class: 20 Kelas Aksara Jawa:\n",
    "- Ba, Ca, Da, Dha, Ga, Ha, Ja, Ka, La, Ma, \n",
    "- Na, Nga, Nya, Pa, Ra, Sa, Ta, Tha, Wa, Ya"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7072e1a6",
   "metadata": {},
   "source": [
    "# Setup Environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deba1394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ec1c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef06ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4253cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install imutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b90208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5278c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea06228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79de6efd",
   "metadata": {},
   "source": [
    "# Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e8d2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt  \n",
    "# import os\n",
    "# from os.path import join\n",
    "# import random\n",
    "# import cv2\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# from tensorflow.keras.utils import to_categorical\n",
    "# from sklearn.model_selection import StratifiedShuffleSplit, train_test_split\n",
    "# from keras.models import Sequential\n",
    "# from tensorflow.keras import layers, models\n",
    "# from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout\n",
    "# from tensorflow.keras.layers import Dense, Flatten, Dropout, BatchNormalization , Activation,Conv2D\n",
    "# from keras.optimizers import Adam\n",
    "# from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "# from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "# from sklearn.metrics import classification_report\n",
    "# import seaborn as sns\n",
    "# from sklearn.metrics import classification_report, accuracy_score,roc_curve,confusion_matrix "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f8462d",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172c2131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import pandas as pd\n",
    "# import cv2\n",
    "\n",
    "# def convert_to_png(image_path, output_path):\n",
    "#     # Read the image using OpenCV\n",
    "#     image = cv2.imread(image_path)\n",
    "#     # Create the output directory if it doesn't exist\n",
    "#     os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "#     # Save the image as PNG\n",
    "#     cv2.imwrite(output_path, image)\n",
    "\n",
    "# def get_last_n_parts(path, n):\n",
    "#     return os.path.join(*path.split(os.sep)[-n:])\n",
    "\n",
    "# def create_labels_csv(DATASET_PATH, OUTPUT_CSV, OUTPUT_IMAGE_DIR):\n",
    "#     # List to store image file paths and their corresponding labels and dimensions\n",
    "#     data = []\n",
    "\n",
    "#     # Traverse the dataset directory\n",
    "#     for root, dirs, files in os.walk(DATASET_PATH):\n",
    "#         for file in files:\n",
    "#             if file.endswith(('.jpg', '.jpeg', '.png')):\n",
    "#                 # Get the class label from the subdirectory name\n",
    "#                 label = os.path.basename(root)\n",
    "#                 # Get the full file path\n",
    "#                 file_path = os.path.join(root, file)\n",
    "                \n",
    "#                 # Create a new file path for the PNG image\n",
    "#                 relative_path = os.path.relpath(file_path, DATASET_PATH)\n",
    "#                 png_file_path = os.path.join(OUTPUT_IMAGE_DIR, os.path.splitext(relative_path)[0] + '.png')\n",
    "                \n",
    "#                 # Convert the image to PNG\n",
    "#                 convert_to_png(file_path, png_file_path)\n",
    "                \n",
    "#                 # Get image dimensions\n",
    "#                 img = cv2.imread(file_path)\n",
    "#                 height, width, _ = img.shape\n",
    "                \n",
    "#                 # Get only the last 3 parts of the path for CSV\n",
    "#                 csv_path = get_last_n_parts(png_file_path, 2)\n",
    "                \n",
    "#                 # Append to the data list including dimensions\n",
    "#                 data.append([csv_path, width, height, label])\n",
    "    \n",
    "#     # Create a DataFrame from the data list\n",
    "#     df = pd.DataFrame(data, columns=['file_path', 'width', 'height', 'label'])\n",
    "    \n",
    "#     # Save the DataFrame to a CSV file\n",
    "#     df.to_csv(OUTPUT_CSV, index=False)\n",
    "#     print(f'Labels CSV file created at: {OUTPUT_CSV}')\n",
    "\n",
    "# # Define the paths for the two datasets and their respective output directories\n",
    "# DATASET_PATH = \"C:\\\\Users\\\\wawn1\\\\projects_skripsi\\\\data\\\\data_original\\\\CustomData\\\\v.7_all_data_custom\\\\\"\n",
    "# OUTPUT_CSV = \"C:\\\\Users\\\\wawn1\\\\projects_skripsi\\\\data\\\\data_preprocessing\\\\v3.4_data_custom\\\\labels.csv\"\n",
    "# OUTPUT_IMAGE_DIR = \"C:\\\\Users\\\\wawn1\\\\projects_skripsi\\\\data\\\\data_preprocessing\\\\v3.4_data_custom\\\\\"\n",
    "\n",
    "# # Process the first dataset\n",
    "# create_labels_csv(DATASET_PATH, OUTPUT_CSV, OUTPUT_IMAGE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad9c781",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Path ke dua file CSV yang berbeda\n",
    "path_main = \"C:\\\\Users\\\\wawn1\\\\projects_skripsi\\\\data\\\\data_preprocessing\\\\v3.4_data_custom\\\\\"\n",
    "file = path_main + \"labels.csv\"\n",
    "df = pd.read_csv(file)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f1ce57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Define function to count files in each directory\n",
    "def count_files(link):\n",
    "    path = link\n",
    "    num_files = len([f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))])\n",
    "    return num_files\n",
    "\n",
    "# Define paths for two sources\n",
    "source_paths = [\n",
    "    \"C:\\\\Users\\\\wawn1\\\\projects_skripsi\\\\data\\\\data_preprocessing\\\\v3.4_data_custom\\\\ba\",\n",
    "    \"C:\\\\Users\\\\wawn1\\\\projects_skripsi\\\\data\\\\data_preprocessing\\\\v3.4_data_custom\\\\ca\",\n",
    "    \"C:\\\\Users\\\\wawn1\\\\projects_skripsi\\\\data\\\\data_preprocessing\\\\v3.4_data_custom\\\\da\",\n",
    "    \"C:\\\\Users\\\\wawn1\\\\projects_skripsi\\\\data\\\\data_preprocessing\\\\v3.4_data_custom\\\\dha\",\n",
    "    \"C:\\\\Users\\\\wawn1\\\\projects_skripsi\\\\data\\\\data_preprocessing\\\\v3.4_data_custom\\\\ga\",\n",
    "    \"C:\\\\Users\\\\wawn1\\\\projects_skripsi\\\\data\\\\data_preprocessing\\\\v3.4_data_custom\\\\ha\",\n",
    "    \"C:\\\\Users\\\\wawn1\\\\projects_skripsi\\\\data\\\\data_preprocessing\\\\v3.4_data_custom\\\\ja\",\n",
    "    \"C:\\\\Users\\\\wawn1\\\\projects_skripsi\\\\data\\\\data_preprocessing\\\\v3.4_data_custom\\\\ka\",\n",
    "    \"C:\\\\Users\\\\wawn1\\\\projects_skripsi\\\\data\\\\data_preprocessing\\\\v3.4_data_custom\\\\la\",\n",
    "    \"C:\\\\Users\\\\wawn1\\\\projects_skripsi\\\\data\\\\data_preprocessing\\\\v3.4_data_custom\\\\ma\",\n",
    "    \"C:\\\\Users\\\\wawn1\\\\projects_skripsi\\\\data\\\\data_preprocessing\\\\v3.4_data_custom\\\\na\",\n",
    "    \"C:\\\\Users\\\\wawn1\\\\projects_skripsi\\\\data\\\\data_preprocessing\\\\v3.4_data_custom\\\\nga\",\n",
    "    \"C:\\\\Users\\\\wawn1\\\\projects_skripsi\\\\data\\\\data_preprocessing\\\\v3.4_data_custom\\\\nya\",\n",
    "    \"C:\\\\Users\\\\wawn1\\\\projects_skripsi\\\\data\\\\data_preprocessing\\\\v3.4_data_custom\\\\pa\",\n",
    "    \"C:\\\\Users\\\\wawn1\\\\projects_skripsi\\\\data\\\\data_preprocessing\\\\v3.4_data_custom\\\\ra\",\n",
    "    \"C:\\\\Users\\\\wawn1\\\\projects_skripsi\\\\data\\\\data_preprocessing\\\\v3.4_data_custom\\\\sa\",\n",
    "    \"C:\\\\Users\\\\wawn1\\\\projects_skripsi\\\\data\\\\data_preprocessing\\\\v3.4_data_custom\\\\ta\",\n",
    "    \"C:\\\\Users\\\\wawn1\\\\projects_skripsi\\\\data\\\\data_preprocessing\\\\v3.4_data_custom\\\\tha\",\n",
    "    \"C:\\\\Users\\\\wawn1\\\\projects_skripsi\\\\data\\\\data_preprocessing\\\\v3.4_data_custom\\\\wa\",\n",
    "    \"C:\\\\Users\\\\wawn1\\\\projects_skripsi\\\\data\\\\data_preprocessing\\\\v3.4_data_custom\\\\ya\"\n",
    "]\n",
    "\n",
    "# Count files for each path\n",
    "file_counts = [count_files(path) for path in source_paths]\n",
    "\n",
    "# Define the input path aksara\n",
    "aksara_categories = ['ba', 'ca', 'da', 'dha', 'ga', 'ha', 'ja', 'ka', 'la', 'Ma', 'na',\n",
    "                     'nga', 'nya', 'pa', 'ra', 'sa', 'ta', 'tha', 'wa', 'ya']\n",
    "\n",
    "colors = ['red', 'orange', 'yellow', 'green', 'blue', 'pink', 'indigo', 'purple', \n",
    "          'cyan', 'magenta', 'lime', 'maroon', 'navy', 'olive', 'orchid', 'peru', \n",
    "          'salmon', 'sienna', 'skyblue', 'tan']  # Colors for each bar\n",
    "\n",
    "# Create bar chart\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.bar(aksara_categories, file_counts, color=colors)\n",
    "plt.xlabel('Kategori Aksara', fontsize=15)\n",
    "plt.ylabel('Jumlah Gambar', fontsize=15)\n",
    "plt.title('Jumlah Gambar Aksara Jawa', fontsize=16)\n",
    "plt.xticks(fontsize=13, rotation=10)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b257451b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the input path and aksaraJowo\n",
    "INPUT_PATH = path_main  \n",
    "AKSARA = aksara_categories\n",
    "\n",
    "# Define the target image size\n",
    "IMAGE_SIZE = (96, 96)\n",
    "INPUT_SHAPE = (96, 96, 1)\n",
    "\n",
    "def create_image_generator(input_path, aksaraJowo, image_size):\n",
    "    for index, jowo in enumerate(aksaraJowo):\n",
    "        aksara_path = os.path.join(input_path, jowo)\n",
    "        \n",
    "        for filename in os.listdir(aksara_path):\n",
    "            image_path = os.path.join(aksara_path, filename)\n",
    "            original_image = cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB)  # Baca dan ubah warna asli\n",
    "            original_image_resized = cv2.resize(original_image, image_size, interpolation=cv2.INTER_AREA)  # Resize gambar asli\n",
    "            grayscale_image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)  # Baca gambar sebagai grayscale\n",
    "            grayscale_image_resized = cv2.resize(grayscale_image, image_size, interpolation=cv2.INTER_AREA)  # Resize gambar grayscale\n",
    "            image_filtered = apply_custom_preprocessing(grayscale_image_resized)\n",
    "            \n",
    "            yield original_image_resized, grayscale_image_resized, image_filtered, index\n",
    "\n",
    "\n",
    "def load_images(input_path, aksaraJowo, image_size):\n",
    "    X_original, X_grayscale, X_filtered, y_label = [], [], [], []\n",
    "    \n",
    "    for original_image, grayscale_image, image_filtered, label in create_image_generator(input_path, aksaraJowo, image_size):\n",
    "        X_original.append(original_image)\n",
    "        X_grayscale.append(grayscale_image)\n",
    "        X_filtered.append(image_filtered)\n",
    "        y_label.append(label)\n",
    "        \n",
    "    X_original = np.array(X_original)\n",
    "    X_grayscale = np.array(X_grayscale)\n",
    "    X_filtered = np.array(X_filtered)\n",
    "    y_label = to_categorical(np.array(y_label))\n",
    "    \n",
    "    return X_original, X_grayscale, X_filtered, y_label\n",
    "\n",
    "def apply_custom_preprocessing(image):\n",
    "    # Apply Gaussian Blur\n",
    "    ApplyGaussian = cv2.GaussianBlur(image, (9, 9), 10.0)\n",
    "    # Enhance the image sharpness\n",
    "    img = cv2.addWeighted(image, 1.5, ApplyGaussian, -0.5, 0, image)\n",
    "    # Apply sharpening filter\n",
    "    kernel = np.array([[-1, -1, -1], [-1, 9, -1], [-1, -1, -1]])\n",
    "    img = cv2.filter2D(img, -1, kernel)\n",
    "    # Remove noise using median filter\n",
    "    img = cv2.medianBlur(img, 1)\n",
    "    # Apply Otsu's thresholding\n",
    "    _, img = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    \n",
    "    # Invert the image (background to black, text to white)\n",
    "    img = 255 - img\n",
    "    \n",
    "    return img\n",
    "\n",
    "# Load images with custom preprocessing applied\n",
    "X_original, X_grayscale, X_filtered, y_label = load_images(INPUT_PATH, AKSARA, IMAGE_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcd0ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select one image to display\n",
    "index_to_display = 100\n",
    "\n",
    "# Display the original image, grayscale image, and filtered image\n",
    "original_image = X_original[index_to_display]\n",
    "grayscale_image = X_grayscale[index_to_display]\n",
    "filtered_image = X_filtered[index_to_display]\n",
    "\n",
    "# Plot the images\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.title('Original Image')\n",
    "plt.imshow(original_image)\n",
    "plt.axis('on')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.title('Grayscale Image')\n",
    "plt.imshow(grayscale_image)\n",
    "plt.axis('on')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.title('Filtered Image')\n",
    "# plt.imshow(filtered_image, cmap='gray')\n",
    "plt.imshow(filtered_image, cmap='gray')\n",
    "plt.axis('on')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be84d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape X_filtered\n",
    "X_filtered_reshaped = X_filtered.reshape(-1, 96, 96, 1).astype(\"float32\") / 255  # Reshape dan normalisasi\n",
    "# X_filtered_reshaped = X_filtered.reshape(-1, 112, 112, 1)\n",
    "\n",
    "print(\"X_filtered shape after reshape:\", X_filtered_reshaped.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d223feaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Tahap pertama: Membagi data menjadi data pelatihan (80%) dan data sementara (20%)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X_filtered_reshaped, y_label, test_size=0.3, random_state=45)\n",
    "\n",
    "# Tahap kedua: Membagi data sementara menjadi data validasi (10%) dan data pengujian (10%)\n",
    "# Data sementara adalah 20% dari total data, jadi 10% / 20% = 0.5 dari data sementara untuk validasi\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=1/3, random_state=37)\n",
    "\n",
    "# Verifikasi ukuran data\n",
    "print(\"Ukuran data pelatihan:\", X_train.shape, y_train.shape)\n",
    "print(\"Ukuran data validasi:\", X_val.shape, y_val.shape)\n",
    "print(\"Ukuran data pengujian:\", X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ab1c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menampilkan salah satu gambar dari data pelatihan\n",
    "plt.imshow(X_train[0], cmap='gray')\n",
    "plt.title(\"Sample Image in Class {} from Training Data !!!\".format(AKSARA[np.argmax(y_train[0])]))\n",
    "plt.axis('on')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1bd922c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menampilkan salah satu gambar dari data validasi\n",
    "plt.imshow(X_val[400], cmap='gray')\n",
    "plt.title(\"Sample Image in Class {} from Validasi Data !!!\".format(AKSARA[np.argmax(y_val[400])]))\n",
    "plt.axis('on')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f3b0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menampilkan salah satu gambar dari data pelatihan\n",
    "plt.imshow(X_test[360], cmap='gray')\n",
    "plt.title(\"Sample Image in Class {} from Validasi Data !!!\".format(AKSARA[np.argmax(y_test[360])]))\n",
    "plt.axis('on')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c1a8ed",
   "metadata": {},
   "source": [
    "# Build the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e967ad7",
   "metadata": {},
   "source": [
    "## Model 1 - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6261c073",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "\n",
    "# Set hyperparameters\n",
    "optimizer = 'SGD'\n",
    "learning_rate = 0.0001\n",
    "dropout_rate = 0.3\n",
    "weight_decay = 0.0001\n",
    "momentum = 0.1\n",
    "clip_norm = 0.0\n",
    "classes = 20\n",
    "input_shape = INPUT_SHAPE\n",
    "\n",
    "def inception_module(x, filters):\n",
    "    f1, f3_r, f3, f5_r, f5, f_pool = filters\n",
    "\n",
    "    path1 = layers.Conv2D(f1, (1, 1), padding='same')(x)\n",
    "    path1 = BatchNormalization()(path1)\n",
    "    path1 = layers.Activation('relu')(path1)\n",
    "\n",
    "    path2 = layers.Conv2D(f3_r, (1, 1), padding='same')(x)\n",
    "    path2 = BatchNormalization()(path2)\n",
    "    path2 = layers.Activation('relu')(path2)\n",
    "    path2 = layers.Conv2D(f3, (3, 3), padding='same')(path2)\n",
    "    path2 = BatchNormalization()(path2)\n",
    "    path2 = layers.Activation('relu')(path2)\n",
    "\n",
    "    path3 = layers.Conv2D(f5_r, (1, 1), padding='same')(x)\n",
    "    path3 = BatchNormalization()(path3)\n",
    "    path3 = layers.Activation('relu')(path3)\n",
    "    path3 = layers.Conv2D(f5, (5, 5), padding='same')(path3)\n",
    "    path3 = BatchNormalization()(path3)\n",
    "    path3 = layers.Activation('relu')(path3)\n",
    "\n",
    "    path4 = layers.MaxPooling2D((3, 3), strides=(1, 1), padding='same')(x)\n",
    "    path4 = layers.Conv2D(f_pool, (1, 1), padding='same')(path4)\n",
    "    path4 = BatchNormalization()(path4)\n",
    "    path4 = layers.Activation('relu')(path4)\n",
    "\n",
    "    return layers.concatenate([path1, path2, path3, path4], axis=-1)\n",
    "\n",
    "def reduction_module(x, k, l, m, n):\n",
    "    path1 = layers.MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)\n",
    "\n",
    "    path2 = layers.Conv2D(n, (3, 3), strides=(2, 2), padding='same')(x)\n",
    "    path2 = BatchNormalization()(path2)\n",
    "    path2 = layers.Activation('relu')(path2)\n",
    "\n",
    "    path3 = layers.Conv2D(k, (1, 1), padding='same')(x)\n",
    "    path3 = BatchNormalization()(path3)\n",
    "    path3 = layers.Activation('relu')(path3)\n",
    "    path3 = layers.Conv2D(l, (3, 3), padding='same')(path3)\n",
    "    path3 = BatchNormalization()(path3)\n",
    "    path3 = layers.Activation('relu')(path3)\n",
    "    path3 = layers.Conv2D(m, (3, 3), strides=(2, 2), padding='same')(path3)\n",
    "    path3 = BatchNormalization()(path3)\n",
    "    path3 = layers.Activation('relu')(path3)\n",
    "\n",
    "    return layers.concatenate([path1, path2, path3], axis=-1)\n",
    "\n",
    "def inception_v3(input_shape, classes, optimizer, learning_rate, dropout_rate, weight_decay, momentum, clip_norm):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "\n",
    "    # Initial layers\n",
    "    x = layers.Conv2D(32, (3, 3), strides=(2, 2), padding='valid')(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.Conv2D(32, (3, 3), padding='valid')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.Conv2D(64, (3, 3), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)\n",
    "    \n",
    "    x = layers.Conv2D(80, (1, 1), padding='valid')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.Conv2D(192, (3, 3), padding='valid')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)\n",
    "\n",
    "    # Inception modules\n",
    "    x = inception_module(x, (64, 48, 64, 64, 96, 32))\n",
    "    x = inception_module(x, (64, 48, 64, 64, 96, 64))\n",
    "    x = inception_module(x, (64, 48, 64, 64, 96, 64))\n",
    "\n",
    "    # Reduction module\n",
    "    x = reduction_module(x, 64, 96, 96, 384)\n",
    "    \n",
    "    # Inception modules\n",
    "    x = inception_module(x, (192, 128, 192, 128, 192, 192))\n",
    "    x = inception_module(x, (192, 160, 192, 160, 192, 192))\n",
    "    x = inception_module(x, (192, 160, 192, 160, 192, 192))\n",
    "    x = inception_module(x, (192, 192, 192, 192, 192, 192))\n",
    "\n",
    "    # Reduction module\n",
    "    x = reduction_module(x, 192, 320, 320, 640)\n",
    "\n",
    "    # Inception modules\n",
    "    x = inception_module(x, (320, 384, 384, 448, 448, 192))\n",
    "    x = inception_module(x, (320, 384, 384, 448, 448, 192))\n",
    "\n",
    "    # Final layers\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    outputs = layers.Dense(classes, activation='softmax')(x)\n",
    "\n",
    "    # Define optimizer with hyperparameters\n",
    "    if optimizer == 'SGD':\n",
    "        optimizer = optimizers.SGD(learning_rate=learning_rate, decay=weight_decay, momentum=momentum, clipnorm=clip_norm)\n",
    "    elif optimizer == 'Adam':\n",
    "        optimizer = optimizers.Adam(learning_rate=learning_rate, decay=weight_decay, momentum=momentum, clipnorm=clip_norm)\n",
    "    elif optimizer == 'RMSprop':\n",
    "        optimizer = optimizers.RMSprop(learning_rate=learning_rate, decay=weight_decay, momentum=momentum, clipnorm=clip_norm)\n",
    "    else:\n",
    "        raise ValueError(\"Optimizer not supported. Choose from 'SGD', 'Adam', or 'RMSprop'.\")\n",
    "\n",
    "    model = models.Model(inputs, outputs)\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Build model\n",
    "model = inception_v3(input_shape=input_shape, classes=classes, optimizer=optimizer, learning_rate=learning_rate, \n",
    "                     dropout_rate=dropout_rate, weight_decay=weight_decay, momentum=momentum, clip_norm=clip_norm)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43e1dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tensorflow.keras.callbacks import Callback, EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# Callback for logging to CSV\n",
    "class CSVLogger(Callback):\n",
    "    def __init__(self, filename):\n",
    "        super(CSVLogger, self).__init__()\n",
    "        self.filename = filename\n",
    "        self.epoch = 1  # Mulai dari epoch 1\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        logs['epoch'] = self.epoch\n",
    "        self.epoch += 1\n",
    "        df = pd.DataFrame([logs])\n",
    "        if epoch == 0:\n",
    "            df.to_csv(self.filename, mode='w', index=False)\n",
    "        else:\n",
    "            df.to_csv(self.filename, mode='a', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b82fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the last epoch from CSV\n",
    "def get_last_epoch(filename):\n",
    "    try:\n",
    "        df = pd.read_csv(filename)\n",
    "        last_epoch = df['epoch'].max()\n",
    "        return last_epoch\n",
    "    except FileNotFoundError:\n",
    "        return 0\n",
    "\n",
    "# Define callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_accuracy', patience=10, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=25, min_lr=1e-4)\n",
    "\n",
    "# Initialize callbacks\n",
    "csv_logger = CSVLogger(\"C:\\\\Users\\\\wawn1\\\\projects_skripsi\\\\save_models\\\\save_models_csv\\\\training_log_v4.3.7_model1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec92240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model and save the logs to CSV\n",
    "hist_v1 = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=35,\n",
    "    batch_size=64,\n",
    "    callbacks=[\n",
    "        reduce_lr,\n",
    "        csv_logger\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee6ea22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Path ke file CSV yang berisi log training\n",
    "csv_file_path = \"C:\\\\Users\\\\wawn1\\\\projects_skripsi\\\\save_models\\\\save_models_csv\\\\training_log_v4.3.7_model1.csv\"\n",
    "\n",
    "# Muat data dari file CSV\n",
    "try:\n",
    "    history_df = pd.read_csv(csv_file_path)\n",
    "except FileNotFoundError:\n",
    "    print(f\"File tidak ditemukan di path: {csv_file_path}\")\n",
    "    history_df = None\n",
    "\n",
    "if history_df is not None:\n",
    "    # Dapatkan jumlah epoch dari panjang data\n",
    "    epochs = range(1, len(history_df) + 1)\n",
    "\n",
    "    # Plot akurasi\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.title('Minggu, 21 Juli - Training Model ', fontsize=15)\n",
    "\n",
    "    # Plot Akurasi Training & Validasi\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, history_df['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(epochs, history_df['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot Loss Training & Validasi\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, history_df['loss'], label='Training Loss')\n",
    "    plt.plot(epochs, history_df['val_loss'], label='Validation Loss')\n",
    "    plt.title('Model Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0409c4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "save_model = \"C:\\\\Users\\\\wawn1\\\\projects_skripsi\\\\save_models\\\\save_models_keras\\\\v4.3.7_model1.keras\"\n",
    "model.save(save_model)\n",
    "\n",
    "# Load model yang telah disimpan\n",
    "new_model = load_model(save_model)\n",
    "\n",
    "# Evaluate the restored model on the test set\n",
    "test_loss, test_acc = new_model.evaluate(X_test, y_test)\n",
    "# test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "\n",
    "print(f\"Test Loss: {test_loss}, Test Accuracy: {test_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e767c45",
   "metadata": {},
   "source": [
    "Perintah dibawah digunakan jika ingin melanjutkan training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64c3123",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import Callback, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import SGD, Adam, RMSprop\n",
    "\n",
    "# Callback for logging to CSV\n",
    "class CSVLogger(Callback):\n",
    "    def __init__(self, filename):\n",
    "        super(CSVLogger, self).__init__()\n",
    "        self.filename = filename\n",
    "        self.epoch = get_last_epoch(filename) + 1  # Mulai dari epoch terakhir yang tersimpan + 1\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        logs['epoch'] = self.epoch\n",
    "        self.epoch += 1\n",
    "        df = pd.DataFrame([logs])\n",
    "        if epoch == 0 and self.epoch == 1:\n",
    "            df.to_csv(self.filename, mode='w', index=False)\n",
    "        else:\n",
    "            df.to_csv(self.filename, mode='a', header=False, index=False)\n",
    "\n",
    "# Function to get the last epoch from CSV\n",
    "def get_last_epoch(filename):\n",
    "    try:\n",
    "        df = pd.read_csv(filename)\n",
    "        last_epoch = df['epoch'].max()\n",
    "        return last_epoch\n",
    "    except FileNotFoundError:\n",
    "        return 0\n",
    "\n",
    "# Load the trained model\n",
    "path_model_v1 = \"C:\\\\Users\\\\wawn1\\\\projects_skripsi\\\\save_models\\\\save_models_keras\\\\v4.3.7_model1.keras\"\n",
    "model = load_model(path_model_v1)\n",
    "\n",
    "# Define callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_accuracy', patience=10, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=55, min_lr=1e-4)\n",
    "\n",
    "# Initialize CSV logger\n",
    "csv_logger = CSVLogger(\"C:\\\\Users\\\\wawn1\\\\projects_skripsi\\\\save_models\\\\save_models_csv\\\\training_log_v4.3.7_model1.csv\")\n",
    "\n",
    "# Get last epoch\n",
    "initial_epoch = get_last_epoch(csv_logger.filename)\n",
    "\n",
    "# Compile the model again (if necessary)\n",
    "optimizer = 'SGD'\n",
    "learning_rate = 0.0001\n",
    "dropout_rate = 0.3\n",
    "weight_decay = 0.0001\n",
    "momentum = 0.1\n",
    "clip_norm = 0.0\n",
    "\n",
    "# Define optimizer with hyperparameters\n",
    "if optimizer == 'SGD':\n",
    "    opt = optimizers.SGD(learning_rate=learning_rate, decay=weight_decay, momentum=momentum, clipnorm=clip_norm)\n",
    "elif optimizer == 'Adam':\n",
    "    opt = optimizers.Adam(learning_rate=learning_rate, decay=weight_decay, momentum=momentum, clipnorm=clip_norm)\n",
    "elif optimizer == 'RMSprop':\n",
    "    opt = optimizers.RMSprop(learning_rate=learning_rate, decay=weight_decay, momentum=momentum, clipnorm=clip_norm)\n",
    "else:\n",
    "    raise ValueError(\"Optimizer not supported. Choose from 'SGD', 'Adam', or 'RMSprop'.\")\n",
    "\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Continue training the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,  # Replace with your training data\n",
    "    validation_data=(X_val, y_val),  # Replace with your validation data\n",
    "    epochs=initial_epoch + 65,  # Continue training for additional epochs\n",
    "    initial_epoch=initial_epoch,\n",
    "    batch_size=64,\n",
    "    callbacks=[\n",
    "        # early_stopping, \n",
    "        reduce_lr, \n",
    "        csv_logger\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.save(path_model_v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb43056",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Path ke file CSV yang berisi log training\n",
    "csv_file_path = \"C:\\\\Users\\\\wawn1\\\\projects_skripsi\\\\save_models\\\\save_models_csv\\\\training_log_v4.3.7_model1.csv\"\n",
    "\n",
    "# Muat data dari file CSV\n",
    "try:\n",
    "    history_df = pd.read_csv(csv_file_path)\n",
    "except FileNotFoundError:\n",
    "    print(f\"File tidak ditemukan di path: {csv_file_path}\")\n",
    "    history_df = None\n",
    "\n",
    "if history_df is not None:\n",
    "    # Dapatkan jumlah epoch dari panjang data\n",
    "    epochs = range(1, len(history_df) + 1)\n",
    "\n",
    "    # Plot akurasi\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.title('Minggu, 21 Juli - Training Model ', fontsize=15)\n",
    "\n",
    "    # Plot Akurasi Training & Validasi\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, history_df['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(epochs, history_df['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot Loss Training & Validasi\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, history_df['loss'], label='Training Loss')\n",
    "    plt.plot(epochs, history_df['val_loss'], label='Validation Loss')\n",
    "    plt.title('Model Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d5533b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, log_loss, accuracy_score, classification_report, confusion_matrix, roc_auc_score, roc_curve, average_precision_score, precision_recall_curve\n",
    "from itertools import cycle\n",
    "\n",
    "# Prediksi menggunakan model pada data uji\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_probabilities = tf.nn.softmax(y_pred).numpy()  # Terapkan softmax secara eksplisit jika perlu\n",
    "y_pred_classes = np.argmax(y_pred_probabilities, axis=1)\n",
    "y_true_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Hitung metrik evaluasi\n",
    "accuracy = accuracy_score(y_true_classes, y_pred_classes)\n",
    "precision = precision_score(y_true_classes, y_pred_classes, average='weighted')\n",
    "recall = recall_score(y_true_classes, y_pred_classes, average='weighted')\n",
    "f1 = f1_score(y_true_classes, y_pred_classes, average='weighted')\n",
    "log_loss_value = log_loss(y_true_classes, y_pred_probabilities)\n",
    "\n",
    "# Print metrik evaluasi\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"Log Loss: {log_loss_value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f4a4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tampilkan classification report yang lengkap\n",
    "report = classification_report(y_true_classes, y_pred_classes, target_names=AKSARA)\n",
    "print(report)\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_true_classes, y_pred_classes)\n",
    "\n",
    "# Normalisasi confusion matrix untuk mendapatkan persentase\n",
    "conf_matrix_normalized = conf_matrix.astype('float') / conf_matrix.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "# Tampilkan confusion matrix sebagai heatmap\n",
    "plt.figure(figsize=(18, 15))\n",
    "sns.heatmap(conf_matrix_normalized, annot=True, fmt=\".2%\", cmap=\"YlGnBu\", xticklabels=AKSARA, yticklabels=AKSARA)\n",
    "plt.xlabel(\"Predicted Aksara\")\n",
    "plt.ylabel(\"Actual Aksara\")\n",
    "plt.title(\"Confusion Matrix (Percentage)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379acbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Tampilkan classification report yang lengkap\n",
    "# report = classification_report(y_true_classes, y_pred_classes, target_names=AKSARA)\n",
    "# print(report)\n",
    "\n",
    "# # Confusion Matrix\n",
    "# conf_matrix = confusion_matrix(y_true_classes, y_pred_classes)\n",
    "\n",
    "# total_per_class = np.sum(conf_matrix, axis=1)\n",
    "\n",
    "# cm_percent = (conf_matrix.astype('float') / total_per_class[:, np.newaxis]) * 100\n",
    "\n",
    "# plt.figure(figsize=(15, 15))\n",
    "# sns.heatmap(cm_percent/np.sum(cm_percent), annot=True, fmt=\".2%\", cmap=\"YlGnBu\", xticklabels=AKSARA, yticklabels=AKSARA)\n",
    "# plt.xlabel(\"Predicted Aksara\")\n",
    "# plt.ylabel(\"Actual Aksara\")\n",
    "# plt.title(\"Confusion Matrix (Percentage)\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb032923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Hitung AUC-ROC untuk setiap kelas\n",
    "# auc_roc_scores = roc_auc_score(y_test, y_pred_probabilities, average=None)\n",
    "# for idx, score in enumerate(auc_roc_scores):\n",
    "#     print(f\"AUC-ROC for class {AKSARA[idx]}: {score:.4f}\")\n",
    "\n",
    "# # Tampilkan ROC Curve untuk setiap kelas\n",
    "# fpr = dict()\n",
    "# tpr = dict()\n",
    "# roc_auc = dict()\n",
    "# n_classes = len(AKSARA)\n",
    "# colors = cycle(['aqua', 'darkorange', 'cornflowerblue', 'green', 'red', 'blue', 'purple', 'pink', 'brown', 'grey', \n",
    "#                 'cyan', 'magenta', 'yellow', 'black', 'lime', 'navy', 'gold', 'teal', 'salmon', 'olive'])\n",
    "\n",
    "# plt.figure(figsize=(10, 8))\n",
    "\n",
    "# for i, color in zip(range(n_classes), colors):\n",
    "#     fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_pred_probabilities[:, i])\n",
    "#     roc_auc[i] = roc_auc_score(y_test[:, i], y_pred_probabilities[:, i])\n",
    "#     plt.plot(fpr[i], tpr[i], color=color, lw=2, label=f'ROC curve of class {AKSARA[i]} (area = {roc_auc[i]:.2f})')\n",
    "\n",
    "# plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "# plt.xlim([0.0, 1.0])\n",
    "# plt.ylim([0.0, 1.05])\n",
    "# plt.xlabel('False Positive Rate')\n",
    "# plt.ylabel('True Positive Rate')\n",
    "# plt.title('Receiver Operating Characteristic (ROC) Curve for each class')\n",
    "# plt.legend(loc=\"lower right\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93130ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Hitung AUPRC untuk setiap kelas\n",
    "# average_precision = dict()\n",
    "# precision_recall_curves = dict()\n",
    "\n",
    "# plt.figure(figsize=(10, 8))\n",
    "\n",
    "# for i, color in zip(range(n_classes), colors):\n",
    "#     precision_recall_curves[i], recall, _ = precision_recall_curve(y_test[:, i], y_pred_probabilities[:, i])\n",
    "#     average_precision[i] = average_precision_score(y_test[:, i], y_pred_probabilities[:, i])\n",
    "#     plt.plot(recall, precision_recall_curves[i], color=color, lw=2, label=f'PR curve of class {AKSARA[i]} (area = {average_precision[i]:.2f})')\n",
    "\n",
    "# plt.xlabel('Recall')\n",
    "# plt.ylabel('Precision')\n",
    "# plt.title('Precision-Recall Curve for each class')\n",
    "# plt.legend(loc=\"lower right\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d36ed66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the function to display images with predictions\n",
    "def display_images_with_predictions(images, true_labels, predictions, labels, max_images=20):\n",
    "    num_images = min(len(images), max_images)\n",
    "    cols = 5\n",
    "    rows = (num_images + cols - 1) // cols  # Compute number of rows needed\n",
    "\n",
    "    plt.figure(figsize=(15, 3 * rows))\n",
    "    \n",
    "    for i in range(num_images):\n",
    "        plt.subplot(rows, cols, i + 1)\n",
    "        plt.imshow(images[i].squeeze(), cmap='gray')\n",
    "        \n",
    "        # Check if prediction is correct\n",
    "        correct = true_labels[i] == predictions[i]\n",
    "        result = \"Correct\" if correct else \"Incorrect\"\n",
    "        color = \"green\" if correct else \"red\"\n",
    "        \n",
    "        # Adjust title to avoid overlapping\n",
    "        plt.title(\n",
    "            f\"True: {labels[true_labels[i]]} | Pred: {labels[predictions[i]]}\\n{result}\",\n",
    "            color=color,\n",
    "            fontsize=15,  # Set font size\n",
    "            pad=10  # Add padding to title\n",
    "        )\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Load the trained model\n",
    "# model_path = \"C:\\\\Users\\\\USER-03\\\\W\\\\projects_s\\\\save_models\\\\save_models_h5\\\\v4.2.7_model1.h5\"\n",
    "model = load_model(path_model_v1)\n",
    "\n",
    "# Assume X_test is already preprocessed and normalized\n",
    "# Perform predictions\n",
    "y_pred_probabilities = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred_probabilities, axis=1)\n",
    "\n",
    "# Convert one-hot encoded y_test to class indices if needed\n",
    "if y_test.ndim == 2:\n",
    "    y_test_classes = np.argmax(y_test, axis=1)\n",
    "else:\n",
    "    y_test_classes = y_test\n",
    "\n",
    "# Display some test images with predictions\n",
    "display_images_with_predictions(X_test, y_test_classes, y_pred_classes, AKSARA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3826113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import cv2\n",
    "# from keras.preprocessing import image\n",
    "# from keras.models import load_model\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Define the custom preprocessing function\n",
    "# def apply_custom_preprocessing(img_array):\n",
    "#     # Convert image to uint8 if not already\n",
    "#     if img_array.dtype != np.uint8:\n",
    "#         img_array = np.uint8(img_array)\n",
    "    \n",
    "#     # Apply Gaussian Blur\n",
    "#     ApplyGaussian = cv2.GaussianBlur(img_array, (9, 9), 10.0)\n",
    "#     # Enhance the image sharpness\n",
    "#     img = cv2.addWeighted(img_array, 1.5, ApplyGaussian, -0.5, 0)\n",
    "#     # Apply sharpening filter\n",
    "#     kernel = np.array([[-1, -1, -1], [-1, 9, -1], [-1, -1, -1]])\n",
    "#     img = cv2.filter2D(img, -1, kernel)\n",
    "#     # Remove noise using median filter\n",
    "#     img = cv2.medianBlur(img, 1)\n",
    "#     # Apply Otsu's thresholding\n",
    "#     _, img = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "#     # Invert the image (background to black, text to white)\n",
    "#     img = 255 - img\n",
    "    \n",
    "#     return img\n",
    "\n",
    "# # Define the function to preprocess the images\n",
    "# def preprocess_images(images, image_size):\n",
    "#     preprocessed_images = []\n",
    "#     for img_array in images:\n",
    "#         # Resize image to the target size\n",
    "#         img_array_resized = cv2.resize(img_array, image_size)\n",
    "#         # Apply custom preprocessing\n",
    "#         img_array_preprocessed = apply_custom_preprocessing(img_array_resized)\n",
    "#         # img_array_preprocessed = img_array_resized\n",
    "#         # Normalize the image\n",
    "#         img_array_preprocessed = img_array_preprocessed.astype(\"float32\") / 255.0\n",
    "#         # Expand dimensions to match input shape\n",
    "#         img_array_preprocessed = np.expand_dims(img_array_preprocessed, axis=-1)\n",
    "#         img_array_preprocessed = np.expand_dims(img_array_preprocessed, axis=0)\n",
    "#         preprocessed_images.append(img_array_preprocessed)\n",
    "    \n",
    "#     return np.vstack(preprocessed_images)\n",
    "\n",
    "# # Define the function to display images with predictions\n",
    "# def display_images_with_predictions(images, true_labels, predictions, labels, max_images=20):\n",
    "#     num_images = min(len(images), max_images)\n",
    "#     cols = 5\n",
    "#     rows = (num_images + cols - 1) // cols  # Compute number of rows needed\n",
    "\n",
    "#     plt.figure(figsize=(15, 3 * rows))\n",
    "    \n",
    "#     for i in range(num_images):\n",
    "#         plt.subplot(rows, cols, i + 1)\n",
    "#         plt.imshow(images[i].squeeze(), cmap='gray')\n",
    "        \n",
    "#         # Check if prediction is correct\n",
    "#         correct = true_labels[i] == predictions[i]\n",
    "#         result = \"Correct\" if correct else \"Incorrect\"\n",
    "#         color = \"green\" if correct else \"red\"\n",
    "        \n",
    "#         # Adjust title to avoid overlapping\n",
    "#         plt.title(\n",
    "#             f\"True: {labels[true_labels[i]]}\\nPred: {labels[predictions[i]]}\\n{result}\",\n",
    "#             color=color,\n",
    "#             fontsize=10,  # Set font size\n",
    "#             pad=10  # Add padding to title\n",
    "#         )\n",
    "#         plt.axis('off')\n",
    "    \n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "# # Load the trained model\n",
    "# # model_path = \"C:\\\\Users\\\\USER-03\\\\W\\\\projects_s\\\\save_models\\\\save_models_h5\\\\v4.2.7_model1.h5\"\n",
    "# model = load_model(path_model_v1)\n",
    "\n",
    "# # Preprocess all images in the test set\n",
    "# image_size = IMAGE_SIZE  # Adjust size if needed\n",
    "# X_test_preprocessed = preprocess_images(X_test, image_size)\n",
    "\n",
    "# # Perform predictions\n",
    "# y_pred_probabilities = model.predict(X_test_preprocessed)\n",
    "# y_pred_classes = np.argmax(y_pred_probabilities, axis=1)\n",
    "\n",
    "# # Convert one-hot encoded y_test to class indices if needed\n",
    "# if y_test.ndim == 2:\n",
    "#     y_test_classes = np.argmax(y_test, axis=1)\n",
    "# else:\n",
    "#     y_test_classes = y_test\n",
    "\n",
    "# # Display some test images with predictions\n",
    "# display_images_with_predictions(X_test, y_test_classes, y_pred_classes, AKSARA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f435a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from keras.preprocessing import image\n",
    "from keras.models import load_model\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the custom preprocessing function\n",
    "def apply_custom_preprocessing(image):\n",
    "    # Convert image to uint8 if not already\n",
    "    if image.dtype != np.uint8:\n",
    "        image = np.uint8(image)\n",
    "    \n",
    "    # Apply Gaussian Blur\n",
    "    ApplyGaussian = cv2.GaussianBlur(image, (9, 9), 10.0)\n",
    "    \n",
    "    # Enhance the image sharpness\n",
    "    img = cv2.addWeighted(image, 1.5, ApplyGaussian, -0.5, 0)\n",
    "    \n",
    "    # Apply sharpening filter\n",
    "    kernel = np.array([[-1, -1, -1], [-1, 9, -1], [-1, -1, -1]])\n",
    "    img = cv2.filter2D(img, -1, kernel)\n",
    "    \n",
    "    # Remove noise using median filter\n",
    "    img = cv2.medianBlur(img, 1)\n",
    "    \n",
    "    # Apply Otsu's thresholding\n",
    "    _, img = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    \n",
    "    # Invert the image (background to black, text to white)\n",
    "    img = 255 - img\n",
    "    \n",
    "    return img\n",
    "\n",
    "# Define the function to preprocess the image\n",
    "def preprocess_image(img_path, image_size):\n",
    "    # Load the image\n",
    "    img = image.load_img(img_path, target_size=image_size, color_mode='grayscale')\n",
    "    # Convert the image to array\n",
    "    img_array = image.img_to_array(img)\n",
    "    \n",
    "    # Apply custom preprocessing\n",
    "    img_array_preprocessed = apply_custom_preprocessing(img_array)\n",
    "    \n",
    "    # Normalize the image\n",
    "    img_array_preprocessed = img_array_preprocessed.astype(\"float32\") / 255.0\n",
    "    # Expand dimensions to match input shape\n",
    "    img_array_preprocessed = np.expand_dims(img_array_preprocessed, axis=-1)\n",
    "    img_array_preprocessed = np.expand_dims(img_array_preprocessed, axis=0)\n",
    "    \n",
    "    return img_array_preprocessed\n",
    "\n",
    "# Load the trained model\n",
    "model = load_model(path_model_v1)  # Replace with your model path\n",
    "\n",
    "# Path to the test image\n",
    "test_image_path = \"C:\\\\Users\\\\wawn1\\\\projects_skripsi\\\\data\\\\data_original\\\\CustomData\\\\data_prediction\\\\ya13.png\"\n",
    "image_size = IMAGE_SIZE\n",
    "\n",
    "# Preprocess the image\n",
    "preprocessed_image = preprocess_image(test_image_path, image_size)\n",
    "\n",
    "# Perform the prediction\n",
    "output = model.predict(preprocessed_image)\n",
    "\n",
    "# Find the index of the maximum value in the output\n",
    "pos = np.argmax(output)\n",
    "\n",
    "# Print the result based on the index\n",
    "print(\"Prediction output:\\n\", output)\n",
    "print(\"Score: \", output[0][pos])\n",
    "print(\"Aksara: \")\n",
    "\n",
    "# Print the class based on the position using if-elif\n",
    "if pos == 0:\n",
    "    print(\"ba\")\n",
    "elif pos == 1:\n",
    "    print('ca')\n",
    "elif pos == 2:\n",
    "    print('da')\n",
    "elif pos == 3:\n",
    "    print('dha')\n",
    "elif pos == 4:\n",
    "    print('ga')\n",
    "elif pos == 5:\n",
    "    print('ha')\n",
    "elif pos == 6:\n",
    "    print('ja')\n",
    "elif pos == 7:\n",
    "    print('ka')\n",
    "elif pos == 8:\n",
    "    print('la')\n",
    "elif pos == 9:\n",
    "    print('ma')\n",
    "elif pos == 10:\n",
    "    print('na')\n",
    "elif pos == 11:\n",
    "    print('nga')\n",
    "elif pos == 12:\n",
    "    print('nya')\n",
    "elif pos == 13:\n",
    "    print('pa')\n",
    "elif pos == 14:\n",
    "    print('ra')\n",
    "elif pos == 15:\n",
    "    print('sa')\n",
    "elif pos == 16:\n",
    "    print('ta')\n",
    "elif pos == 17:\n",
    "    print('tha')\n",
    "elif pos == 18:\n",
    "    print('wa')\n",
    "elif pos == 19:\n",
    "    print('ya')\n",
    "\n",
    "# Display the preprocessed image with prediction\n",
    "plt.figure(figsize=(5, 5))\n",
    "\n",
    "# Preprocessed Image\n",
    "preprocessed_for_display = apply_custom_preprocessing(image.img_to_array(image.load_img(test_image_path, target_size=image_size, color_mode='grayscale')))\n",
    "plt.imshow(preprocessed_for_display.squeeze(), cmap='gray')\n",
    "# plt.title(f\"Predicted: {predicted_label} || Score: {output[0][pos]:.4f}\")\n",
    "plt.axis('on')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89dc25d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from keras.preprocessing import image\n",
    "from keras.models import load_model\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the custom preprocessing function\n",
    "def apply_custom_preprocessing(image):\n",
    "    # Convert image to uint8 if not already\n",
    "    if image.dtype != np.uint8:\n",
    "        image = np.uint8(image)\n",
    "    \n",
    "    # Apply Gaussian Blur\n",
    "    ApplyGaussian = cv2.GaussianBlur(image, (9, 9), 10.0)\n",
    "    \n",
    "    # Enhance the image sharpness\n",
    "    img = cv2.addWeighted(image, 1.5, ApplyGaussian, -0.5, 0)\n",
    "    \n",
    "    # Apply sharpening filter\n",
    "    kernel = np.array([[-1, -1, -1], [-1, 9, -1], [-1, -1, -1]])\n",
    "    img = cv2.filter2D(img, -1, kernel)\n",
    "    \n",
    "    # Remove noise using median filter\n",
    "    img = cv2.medianBlur(img, 1)\n",
    "    \n",
    "    # Apply Otsu's thresholding\n",
    "    _, img = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    \n",
    "    # Invert the image (background to black, text to white)\n",
    "    img = 255 - img\n",
    "    \n",
    "    return img\n",
    "\n",
    "# Define the function to preprocess the image\n",
    "def preprocess_image(img_path, image_size):\n",
    "    # Load the image\n",
    "    img = image.load_img(img_path, target_size=image_size, color_mode='grayscale')\n",
    "    # Convert the image to array\n",
    "    img_array = image.img_to_array(img)\n",
    "    \n",
    "    # Apply custom preprocessing\n",
    "    img_array_preprocessed = apply_custom_preprocessing(img_array)\n",
    "    \n",
    "    # Normalize the image\n",
    "    img_array_preprocessed = img_array_preprocessed.astype(\"float32\") / 255.0\n",
    "    # Expand dimensions to match input shape\n",
    "    img_array_preprocessed = np.expand_dims(img_array_preprocessed, axis=-1)\n",
    "    img_array_preprocessed = np.expand_dims(img_array_preprocessed, axis=0)\n",
    "    \n",
    "    return img_array_preprocessed, img_array\n",
    "\n",
    "# Load the trained model\n",
    "model = load_model(path_model_v1)  # Replace with your model path\n",
    "\n",
    "# Path to the test image\n",
    "test_image_path = \"C:\\\\Users\\\\wawn1\\\\projects_skripsi\\\\data\\\\data_original\\\\CustomData\\\\data_prediction\\\\pa10.png\"\n",
    "image_size = IMAGE_SIZE\n",
    "\n",
    "# Preprocess the image\n",
    "preprocessed_image, original_image = preprocess_image(test_image_path, image_size)\n",
    "\n",
    "# Perform the prediction\n",
    "output = model.predict(preprocessed_image)\n",
    "\n",
    "# Find the index of the maximum value in the output\n",
    "max_val = output[0][0]\n",
    "pos = 0\n",
    "for i in range(1, len(output[0])):\n",
    "    if output[0][i] > max_val:\n",
    "        max_val = output[0][i]\n",
    "        pos = i\n",
    "\n",
    "# Print the result based on the index\n",
    "print(\"Prediction output:\\n\", output)\n",
    "print(\"Score: \", max_val)\n",
    "\n",
    "# Define labels\n",
    "labels = ['ba', 'ca', 'da', 'dha', 'ga', 'ha', 'ja', 'ka', 'la', 'ma', \n",
    "          'na', 'nga', 'nya', 'pa', 'ra', 'sa', 'ta', 'tha', 'wa', 'ya']\n",
    "predicted_label = labels[pos]\n",
    "print(\"Aksara: \", predicted_label)\n",
    "\n",
    "# Display the preprocessed image with prediction\n",
    "plt.figure(figsize=(5, 5))\n",
    "\n",
    "# Preprocessed Image\n",
    "preprocessed_for_display = apply_custom_preprocessing(image.img_to_array(image.load_img(test_image_path, target_size=image_size, color_mode='grayscale')))\n",
    "plt.imshow(preprocessed_for_display.squeeze(), cmap='gray')\n",
    "plt.title(f\"Predicted: {predicted_label} || Score: {max_val:.4f}\")\n",
    "plt.axis('on')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729a5d1e",
   "metadata": {},
   "source": [
    "## Model 2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "c041243b233ff18225279d670b7eabf2d77dba2a8c99e509ee6b49150d1580ae"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
