{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c03f9bba",
   "metadata": {},
   "source": [
    "# Note\n",
    "1. Melakukan percobaan build_models v4.0\n",
    "2. Dilakukan pada hari Senin, 22 Juli 2024\n",
    "3. Tempat di Kost\n",
    "4. Data menggunakan Dataset yang dari Roboflow \n",
    "4. Meliputi Class: 20 Kelas Aksara Jawa:\n",
    "- Ba, Ca, Da, Dha, Ga, Ha, Ja, Ka, La, Ma, \n",
    "- Na, Nga, Nya, Pa, Ra, Sa, Ta, Tha, Wa, Ya"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7072e1a6",
   "metadata": {},
   "source": [
    "# Setup Environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deba1394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ec1c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef06ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4253cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install imutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b90208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5278c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea06228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79de6efd",
   "metadata": {},
   "source": [
    "# Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e8d2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt  \n",
    "# import os\n",
    "# from os.path import join\n",
    "# import random\n",
    "# import cv2\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# from tensorflow.keras.utils import to_categorical\n",
    "# from sklearn.model_selection import StratifiedShuffleSplit, train_test_split\n",
    "# from keras.models import Sequential\n",
    "# from tensorflow.keras import layers, models\n",
    "# from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout\n",
    "# from tensorflow.keras.layers import Dense, Flatten, Dropout, BatchNormalization , Activation,Conv2D\n",
    "# from keras.optimizers import Adam\n",
    "# from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "# from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "# from sklearn.metrics import classification_report\n",
    "# import seaborn as sns\n",
    "# from sklearn.metrics import classification_report, accuracy_score,roc_curve,confusion_matrix "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f8462d",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172c2131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import pandas as pd\n",
    "# import cv2\n",
    "\n",
    "# def convert_to_png(image_path, output_path):\n",
    "#     # Read the image using OpenCV\n",
    "#     image = cv2.imread(image_path)\n",
    "#     # Create the output directory if it doesn't exist\n",
    "#     os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "#     # Save the image as PNG\n",
    "#     cv2.imwrite(output_path, image)\n",
    "\n",
    "# def get_last_n_parts(path, n):\n",
    "#     return os.path.join(*path.split(os.sep)[-n:])\n",
    "\n",
    "# def create_labels_csv(DATASET_PATH, OUTPUT_CSV, OUTPUT_IMAGE_DIR):\n",
    "#     # List to store image file paths and their corresponding labels and dimensions\n",
    "#     data = []\n",
    "\n",
    "#     # Traverse the dataset directory\n",
    "#     for root, dirs, files in os.walk(DATASET_PATH):\n",
    "#         for file in files:\n",
    "#             if file.endswith(('.jpg', '.jpeg', '.png')):\n",
    "#                 # Get the class label from the subdirectory name\n",
    "#                 label = os.path.basename(root)\n",
    "#                 # Get the full file path\n",
    "#                 file_path = os.path.join(root, file)\n",
    "                \n",
    "#                 # Create a new file path for the PNG image\n",
    "#                 relative_path = os.path.relpath(file_path, DATASET_PATH)\n",
    "#                 png_file_path = os.path.join(OUTPUT_IMAGE_DIR, os.path.splitext(relative_path)[0] + '.png')\n",
    "                \n",
    "#                 # Convert the image to PNG\n",
    "#                 convert_to_png(file_path, png_file_path)\n",
    "                \n",
    "#                 # Get image dimensions\n",
    "#                 img = cv2.imread(file_path)\n",
    "#                 height, width, _ = img.shape\n",
    "                \n",
    "#                 # Get only the last 3 parts of the path for CSV\n",
    "#                 csv_path = get_last_n_parts(png_file_path, 2)\n",
    "                \n",
    "#                 # Append to the data list including dimensions\n",
    "#                 data.append([csv_path, width, height, label])\n",
    "    \n",
    "#     # Create a DataFrame from the data list\n",
    "#     df = pd.DataFrame(data, columns=['file_path', 'width', 'height', 'label'])\n",
    "    \n",
    "#     # Save the DataFrame to a CSV file\n",
    "#     df.to_csv(OUTPUT_CSV, index=False)\n",
    "#     print(f'Labels CSV file created at: {OUTPUT_CSV}')\n",
    "\n",
    "# # Define the paths for the two datasets and their respective output directories\n",
    "# DATASET_PATH = \"C:\\\\Users\\\\wawn1\\\\projects_skripsi\\\\data\\\\data_original\\\\CustomData\\\\v.6_all_data_custom\\\\\"\n",
    "# OUTPUT_CSV = \"C:\\\\Users\\\\wawn1\\\\projects_skripsi\\\\data\\\\data_preprocessing\\\\v3.1_all_data_custom\\\\labels.csv\"\n",
    "# OUTPUT_IMAGE_DIR = \"C:\\\\Users\\\\wawn1\\\\projects_skripsi\\\\data\\\\data_preprocessing\\\\v3.1_all_data_custom\\\\\"\n",
    "\n",
    "# # Process the first dataset\n",
    "# create_labels_csv(DATASET_PATH, OUTPUT_CSV, OUTPUT_IMAGE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad9c781",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Path ke dua file CSV yang berbeda\n",
    "path_main = \"C:\\\\Users\\\\wawn1\\\\projects_skripsi\\\\data\\\\data_preprocessing\\\\v3.4_data_custom\\\\\"\n",
    "file = path_main + \"labels.csv\"\n",
    "df = pd.read_csv(file)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f1ce57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Define function to count files in each directory\n",
    "def count_files(link):\n",
    "    path = link\n",
    "    num_files = len([f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))])\n",
    "    return num_files\n",
    "\n",
    "# Define paths for two sources\n",
    "source_paths = [\n",
    "    \"C:\\\\Users\\\\wawn1\\\\projects_skripsi\\\\data\\\\data_preprocessing\\\\v3.4_data_custom\\\\ba\",\n",
    "    \"C:\\\\Users\\\\wawn1\\\\projects_skripsi\\\\data\\\\data_preprocessing\\\\v3.4_data_custom\\\\ca\",\n",
    "    \"C:\\\\Users\\\\wawn1\\\\projects_skripsi\\\\data\\\\data_preprocessing\\\\v3.4_data_custom\\\\da\",\n",
    "    \"C:\\\\Users\\\\wawn1\\\\projects_skripsi\\\\data\\\\data_preprocessing\\\\v3.4_data_custom\\\\dha\",\n",
    "    \"C:\\\\Users\\\\wawn1\\\\projects_skripsi\\\\data\\\\data_preprocessing\\\\v3.4_data_custom\\\\ga\",\n",
    "    \"C:\\\\Users\\\\wawn1\\\\projects_skripsi\\\\data\\\\data_preprocessing\\\\v3.4_data_custom\\\\ha\",\n",
    "    \"C:\\\\Users\\\\wawn1\\\\projects_skripsi\\\\data\\\\data_preprocessing\\\\v3.4_data_custom\\\\ja\",\n",
    "    \"C:\\\\Users\\\\wawn1\\\\projects_skripsi\\\\data\\\\data_preprocessing\\\\v3.4_data_custom\\\\ka\",\n",
    "    \"C:\\\\Users\\\\wawn1\\\\projects_skripsi\\\\data\\\\data_preprocessing\\\\v3.4_data_custom\\\\la\",\n",
    "    \"C:\\\\Users\\\\wawn1\\\\projects_skripsi\\\\data\\\\data_preprocessing\\\\v3.4_data_custom\\\\ma\",\n",
    "    \"C:\\\\Users\\\\wawn1\\\\projects_skripsi\\\\data\\\\data_preprocessing\\\\v3.4_data_custom\\\\na\",\n",
    "    \"C:\\\\Users\\\\wawn1\\\\projects_skripsi\\\\data\\\\data_preprocessing\\\\v3.4_data_custom\\\\nga\",\n",
    "    \"C:\\\\Users\\\\wawn1\\\\projects_skripsi\\\\data\\\\data_preprocessing\\\\v3.4_data_custom\\\\nya\",\n",
    "    \"C:\\\\Users\\\\wawn1\\\\projects_skripsi\\\\data\\\\data_preprocessing\\\\v3.4_data_custom\\\\pa\",\n",
    "    \"C:\\\\Users\\\\wawn1\\\\projects_skripsi\\\\data\\\\data_preprocessing\\\\v3.4_data_custom\\\\ra\",\n",
    "    \"C:\\\\Users\\\\wawn1\\\\projects_skripsi\\\\data\\\\data_preprocessing\\\\v3.4_data_custom\\\\sa\",\n",
    "    \"C:\\\\Users\\\\wawn1\\\\projects_skripsi\\\\data\\\\data_preprocessing\\\\v3.4_data_custom\\\\ta\",\n",
    "    \"C:\\\\Users\\\\wawn1\\\\projects_skripsi\\\\data\\\\data_preprocessing\\\\v3.4_data_custom\\\\tha\",\n",
    "    \"C:\\\\Users\\\\wawn1\\\\projects_skripsi\\\\data\\\\data_preprocessing\\\\v3.4_data_custom\\\\wa\",\n",
    "    \"C:\\\\Users\\\\wawn1\\\\projects_skripsi\\\\data\\\\data_preprocessing\\\\v3.4_data_custom\\\\ya\"\n",
    "]\n",
    "\n",
    "# Count files for each path\n",
    "file_counts = [count_files(path) for path in source_paths]\n",
    "\n",
    "# Define the input path aksara\n",
    "aksara_categories = ['ba', 'ca', 'da', 'dha', 'ga', 'ha', 'ja', 'ka', 'la', 'Ma', 'na',\n",
    "                     'nga', 'nya', 'pa', 'ra', 'sa', 'ta', 'tha', 'wa', 'ya']\n",
    "\n",
    "colors = ['red', 'orange', 'yellow', 'green', 'blue', 'pink', 'indigo', 'purple', \n",
    "          'cyan', 'magenta', 'lime', 'maroon', 'navy', 'olive', 'orchid', 'peru', \n",
    "          'salmon', 'sienna', 'skyblue', 'tan']  # Colors for each bar\n",
    "\n",
    "# Create bar chart\n",
    "plt.figure(figsize=(9, 4))\n",
    "plt.bar(aksara_categories, file_counts, color=colors)\n",
    "plt.xlabel('Kategori Aksara', fontsize=14)\n",
    "plt.ylabel('Jumlah Gambar', fontsize=14)\n",
    "plt.title('Jumlah Gambar Aksara Jawa', fontsize=16)\n",
    "plt.xticks(fontsize=10, rotation=10)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b257451b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the input path and aksaraJowo\n",
    "INPUT_PATH = path_main  \n",
    "AKSARA = aksara_categories\n",
    "\n",
    "# Define the target image size\n",
    "IMAGE_SIZE = (96, 96)\n",
    "INPUT_SHAPE = (96, 96, 1)\n",
    "\n",
    "def create_image_generator(input_path, aksaraJowo, image_size):\n",
    "    for index, jowo in enumerate(aksaraJowo):\n",
    "        aksara_path = os.path.join(input_path, jowo)\n",
    "        \n",
    "        for filename in os.listdir(aksara_path):\n",
    "            image_path = os.path.join(aksara_path, filename)\n",
    "            original_image = cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB)  # Baca dan ubah warna asli\n",
    "            original_image_resized = cv2.resize(original_image, image_size, interpolation=cv2.INTER_AREA)  # Resize gambar asli\n",
    "            grayscale_image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)  # Baca gambar sebagai grayscale\n",
    "            grayscale_image_resized = cv2.resize(grayscale_image, image_size, interpolation=cv2.INTER_AREA)  # Resize gambar grayscale\n",
    "            image_filtered = apply_custom_preprocessing(grayscale_image_resized)\n",
    "            \n",
    "            yield original_image_resized, grayscale_image_resized, image_filtered, index\n",
    "\n",
    "\n",
    "def load_images(input_path, aksaraJowo, image_size):\n",
    "    X_original, X_grayscale, X_filtered, y_label = [], [], [], []\n",
    "    \n",
    "    for original_image, grayscale_image, image_filtered, label in create_image_generator(input_path, aksaraJowo, image_size):\n",
    "        X_original.append(original_image)\n",
    "        X_grayscale.append(grayscale_image)\n",
    "        X_filtered.append(image_filtered)\n",
    "        y_label.append(label)\n",
    "        \n",
    "    X_original = np.array(X_original)\n",
    "    X_grayscale = np.array(X_grayscale)\n",
    "    X_filtered = np.array(X_filtered)\n",
    "    y_label = to_categorical(np.array(y_label))\n",
    "    \n",
    "    return X_original, X_grayscale, X_filtered, y_label\n",
    "\n",
    "def apply_custom_preprocessing(image):\n",
    "    # Apply Gaussian Blur\n",
    "    ApplyGaussian = cv2.GaussianBlur(image, (9, 9), 10.0)\n",
    "    # Enhance the image sharpness\n",
    "    img = cv2.addWeighted(image, 1.5, ApplyGaussian, -0.5, 0, image)\n",
    "    # Apply sharpening filter\n",
    "    kernel = np.array([[-1, -1, -1], [-1, 9, -1], [-1, -1, -1]])\n",
    "    img = cv2.filter2D(img, -1, kernel)\n",
    "    # Remove noise using median filter\n",
    "    img = cv2.medianBlur(img, 1)\n",
    "    # Apply Otsu's thresholding\n",
    "    _, img = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    \n",
    "    # Invert the image (background to black, text to white)\n",
    "    img = 255 - img\n",
    "    \n",
    "    return img\n",
    "\n",
    "# Load images with custom preprocessing applied\n",
    "X_original, X_grayscale, X_filtered, y_label = load_images(INPUT_PATH, AKSARA, IMAGE_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcd0ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select one image to display\n",
    "index_to_display = 100\n",
    "\n",
    "# Display the original image, grayscale image, and filtered image\n",
    "original_image = X_original[index_to_display]\n",
    "grayscale_image = X_grayscale[index_to_display]\n",
    "filtered_image = X_filtered[index_to_display]\n",
    "\n",
    "# Plot the images\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.title('Original Image')\n",
    "plt.imshow(original_image)\n",
    "plt.axis('on')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.title('Grayscale Image')\n",
    "plt.imshow(grayscale_image)\n",
    "plt.axis('on')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.title('Filtered Image')\n",
    "# plt.imshow(filtered_image, cmap='gray')\n",
    "plt.imshow(filtered_image, cmap='gray')\n",
    "plt.axis('on')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be84d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape X_filtered\n",
    "X_filtered_reshaped = X_filtered.reshape(-1, 96, 96, 1).astype(\"float32\") / 255  # Reshape dan normalisasi\n",
    "\n",
    "print(\"X_filtered shape after reshape:\", X_filtered_reshaped.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d223feaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Tahap pertama: Membagi data menjadi data pelatihan (70%) dan data sementara (30%)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X_filtered_reshaped, y_label, test_size=0.3, random_state=45)\n",
    "\n",
    "# Tahap kedua: Membagi data sementara menjadi data validasi (20%) dan data pengujian (10%)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=1/3, random_state=37)\n",
    "\n",
    "# Verifikasi ukuran data\n",
    "print(\"Ukuran data pelatihan:\", X_train.shape, y_train.shape)\n",
    "print(\"Ukuran data validasi:\", X_val.shape, y_val.shape)\n",
    "print(\"Ukuran data pengujian:\", X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ab1c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menampilkan salah satu gambar dari data pelatihan\n",
    "plt.imshow(X_train[0], cmap='gray')\n",
    "plt.title(\"Sample Image in Class {} from Training Data !!!\".format(AKSARA[np.argmax(y_train[0])]))\n",
    "plt.axis('on')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1bd922c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menampilkan salah satu gambar dari data validasi\n",
    "plt.imshow(X_val[0], cmap='gray')\n",
    "plt.title(\"Sample Image in Class {} from Validasi Data !!!\".format(AKSARA[np.argmax(y_val[0])]))\n",
    "plt.axis('on')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f3b0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menampilkan salah satu gambar dari data pelatihan\n",
    "plt.imshow(X_test[0], cmap='gray')\n",
    "plt.title(\"Sample Image in Class {} from Validasi Data !!!\".format(AKSARA[np.argmax(y_test[0])]))\n",
    "plt.axis('on')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c1a8ed",
   "metadata": {},
   "source": [
    "# Build the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e967ad7",
   "metadata": {},
   "source": [
    "## Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3df384",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout, BatchNormalization\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "# Set hyperparameters tuning\n",
    "optimizer = 'SGD'\n",
    "learning_rate = 0.0001\n",
    "dropout_rate = 0.3\n",
    "weight_decay = 0.0001\n",
    "momentum = 0.9\n",
    "clip_norm = 0.0\n",
    "num_classes = 20\n",
    "input_shape = INPUT_SHAPE\n",
    "\n",
    "def create_model(input_shape, num_classes, optimizer_name, learning_rate, dropout_rate, weight_decay, momentum, clip_norm):\n",
    "    model = Sequential()\n",
    "\n",
    "    # Block 1\n",
    "    model.add(Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=input_shape))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(32, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "\n",
    "    # Block 2\n",
    "    model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "\n",
    "    # Block 3\n",
    "    model.add(Conv2D(128, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(128, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(128, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "\n",
    "    # Block 4\n",
    "    model.add(Conv2D(256, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(256, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(256, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "\n",
    "    # Block 5\n",
    "    model.add(Conv2D(512, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(512, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(512, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "\n",
    "    # Flatten\n",
    "    model.add(Flatten())\n",
    "\n",
    "    # Dense layers\n",
    "    model.add(Dense(1024, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(dropout_rate))\n",
    "\n",
    "    # Output layer\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    # Define optimizer with hyperparameters\n",
    "    if optimizer_name == 'SGD':\n",
    "        optimizer = optimizers.SGD(learning_rate=learning_rate, decay=weight_decay, momentum=momentum, clipnorm=clip_norm)\n",
    "    elif optimizer_name == 'Adam':\n",
    "        optimizer = optimizers.Adam(learning_rate=learning_rate, decay=weight_decay, clipnorm=clip_norm)\n",
    "    elif optimizer_name == 'RMSprop':\n",
    "        optimizer = optimizers.RMSprop(learning_rate=learning_rate, decay=weight_decay, clipnorm=clip_norm)\n",
    "    else:\n",
    "        raise ValueError(\"Optimizer not supported. Choose from 'SGD', 'Adam', or 'RMSprop'.\")\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "# Create the model\n",
    "model = create_model(input_shape=input_shape, num_classes=num_classes, optimizer_name=optimizer, learning_rate=learning_rate, \n",
    "                     dropout_rate=dropout_rate, weight_decay=weight_decay, momentum=momentum, clip_norm=clip_norm)\n",
    "\n",
    "# Print model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43e1dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import Callback, EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "# Callback for logging to CSV\n",
    "class CustomCSVLogger(Callback):\n",
    "    def __init__(self, filename):\n",
    "        super(CustomCSVLogger, self).__init__()\n",
    "        self.filename = filename\n",
    "        self.epoch = 1\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        logs['epoch'] = self.epoch\n",
    "        self.epoch += 1\n",
    "        df = pd.DataFrame([logs])\n",
    "        if epoch == 0:\n",
    "            df.to_csv(self.filename, mode='w', index=False)\n",
    "        else:\n",
    "            df.to_csv(self.filename, mode='a', header=False, index=False)\n",
    "\n",
    "# Path file CSVlogger and file load trained model\n",
    "csv_logger_v1 = \"C:\\\\Users\\\\USER\\\\projects_waw\\\\projects_skripsi\\\\save_models\\\\save_models_csv\\\\v4.3.9_log_1.csv\"\n",
    "model_filepath_v1 = \"C:\\\\Users\\\\USER\\\\projects_waw\\\\projects_skripsi\\\\save_models\\\\save_models_keras\\\\v4.3.9_keras_1.keras\"\n",
    "\n",
    "# Define callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_accuracy', patience=10, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss',factor=0.2, patience=25, min_lr=learning_rate)\n",
    "\n",
    "# Definisikan callback CSVLogger\n",
    "csv_logger = CustomCSVLogger(csv_logger_v1)\n",
    "\n",
    "# Definisikan callback ModelCheckpoint\n",
    "model_checkpoint = ModelCheckpoint(filepath=model_filepath_v1, monitor='val_accuracy', verbose=0, save_best_only=True, save_weights_only=False, mode='auto')\n",
    "\n",
    "# Train the model and save the logs to CSV\n",
    "hist_v1 = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=5,\n",
    "    batch_size=64,\n",
    "    callbacks=[\n",
    "        # early_stopping,\n",
    "        reduce_lr,\n",
    "        model_checkpoint,\n",
    "        csv_logger\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Muat data dari file CSV\n",
    "try:\n",
    "    history_df = pd.read_csv(csv_logger_v1)\n",
    "except FileNotFoundError:\n",
    "    print(f\"File tidak ditemukan di path: {csv_logger_v1}\")\n",
    "    history_df = None\n",
    "\n",
    "if history_df is not None:\n",
    "    # Dapatkan jumlah epoch dari panjang data\n",
    "    epochs = range(1, len(history_df) + 1)\n",
    "\n",
    "    # Plot akurasi\n",
    "    plt.figure(figsize=(15, 5))\n",
    "\n",
    "    # Plot Akurasi Training & Validasi\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, history_df['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(epochs, history_df['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot Loss Training & Validasi\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, history_df['loss'], label='Training Loss')\n",
    "    plt.plot(epochs, history_df['val_loss'], label='Validation Loss')\n",
    "    plt.title('Model Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Load the trained model\n",
    "best_model = load_model(model_filepath_v1)\n",
    "\n",
    "# Evaluate the restored model on the test set\n",
    "loss, accuracy = best_model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {loss}, Test Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb18dbc",
   "metadata": {},
   "source": [
    "Code dibawah digunakan untuk melanjutkan training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8667c131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.models import load_model\n",
    "# from tensorflow.keras.callbacks import Callback, EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "# from tensorflow.keras.optimizers import SGD, Adam, RMSprop\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Callback for logging to CSV\n",
    "# class CSVLogger(Callback):\n",
    "#     def __init__(self, filename):\n",
    "#         super(CSVLogger, self).__init__()\n",
    "#         self.filename = filename\n",
    "#         self.epoch = get_last_epoch(filename) + 1  \n",
    "\n",
    "#     def on_epoch_end(self, epoch, logs=None):\n",
    "#         logs = logs or {}\n",
    "#         logs['epoch'] = self.epoch\n",
    "#         self.epoch += 1\n",
    "#         df = pd.DataFrame([logs])\n",
    "#         if epoch == 0 and self.epoch == 1:\n",
    "#             df.to_csv(self.filename, mode='w', index=False)\n",
    "#         else:\n",
    "#             df.to_csv(self.filename, mode='a', header=False, index=False)\n",
    "\n",
    "# # Function to get the last epoch from CSV\n",
    "# def get_last_epoch(filename):\n",
    "#     try:\n",
    "#         df = pd.read_csv(filename)\n",
    "#         last_epoch = df['epoch'].max()\n",
    "#         return last_epoch\n",
    "#     except FileNotFoundError:\n",
    "#         return 0\n",
    "\n",
    "# # Path file CSVlogger and file load trained model\n",
    "# csv_logger_v1 = \"C:\\\\Users\\\\wawn1\\\\projects_skripsi\\\\save_models\\\\save_models_csv\\\\v4.3.6_log_1.csv\"\n",
    "# model_filepath_v1 = \"C:\\\\Users\\\\wawn1\\\\projects_skripsi\\\\save_models\\\\save_models_keras\\\\v4.3.6_keras_1.keras\"\n",
    "\n",
    "# # Initialize CSV logger\n",
    "# csv_logger = CustomCSVLogger(csv_logger_v1)\n",
    "\n",
    "# # Load the trained model\n",
    "# model = load_model(model_filepath_v1)\n",
    "\n",
    "# # Define callbacks\n",
    "# early_stopping = EarlyStopping(monitor='val_accuracy', patience=10, restore_best_weights=True)\n",
    "# reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=25, min_lr=1e-4)\n",
    "# model_checkpoint = ModelCheckpoint(filepath=model_filepath_v1, monitor='val_accuracy', verbose=0, save_best_only=True, save_weights_only=False, mode='auto')\n",
    "\n",
    "# # Compile the model again (if necessary)\n",
    "# optimizer = 'SGD'\n",
    "# learning_rate = 0.0001\n",
    "# dropout_rate = 0.3\n",
    "# weight_decay = 0.0001\n",
    "# momentum = 0.9\n",
    "# clip_norm = 0.0\n",
    "\n",
    "# # Define optimizer with hyperparameters\n",
    "# if optimizer == 'SGD':\n",
    "#     opt = SGD(learning_rate=learning_rate, decay=weight_decay, momentum=momentum, clipnorm=clip_norm)\n",
    "# elif optimizer == 'Adam':\n",
    "#     opt = Adam(learning_rate=learning_rate, decay=weight_decay, clipnorm=clip_norm)\n",
    "# elif optimizer == 'RMSprop':\n",
    "#     opt = RMSprop(learning_rate=learning_rate, decay=weight_decay, clipnorm=clip_norm)\n",
    "# else:\n",
    "#     raise ValueError(\"Optimizer not supported. Choose from 'SGD', 'Adam', or 'RMSprop'.\")\n",
    "\n",
    "# # Compile model\n",
    "# model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# # Print model summary\n",
    "# # model.summary()\n",
    "\n",
    "# # Get last epoch\n",
    "# initial_epoch = get_last_epoch(csv_logger.filename)\n",
    "\n",
    "# # Continue training the model\n",
    "# hist_v1 = model.fit(\n",
    "#     X_train, y_train,  # Replace with your training data\n",
    "#     validation_data=(X_val, y_val),  # Replace with your validation data\n",
    "#     initial_epoch=initial_epoch,\n",
    "#     epochs=initial_epoch + 1,  \n",
    "#     batch_size=64,\n",
    "#     callbacks=[\n",
    "#         # early_stopping,\n",
    "#         reduce_lr,\n",
    "#         model_checkpoint,\n",
    "#         csv_logger\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# # Muat data dari file CSV\n",
    "# try:\n",
    "#     history_df = pd.read_csv(csv_logger_v1)\n",
    "# except FileNotFoundError:\n",
    "#     print(f\"File tidak ditemukan di path: {csv_logger_v1}\")\n",
    "#     history_df = None\n",
    "\n",
    "# if history_df is not None:\n",
    "#     # Dapatkan jumlah epoch dari panjang data\n",
    "#     epochs = range(1, len(history_df) + 1)\n",
    "\n",
    "#     # Plot akurasi\n",
    "#     plt.figure(figsize=(12, 4))\n",
    "#     plt.title('Senis, 22 Juli - Training Model', fontsize=15)\n",
    "\n",
    "#     # Plot Akurasi Training & Validasi\n",
    "#     plt.subplot(1, 2, 1)\n",
    "#     plt.plot(epochs, history_df['accuracy'], label='Training Accuracy')\n",
    "#     plt.plot(epochs, history_df['val_accuracy'], label='Validation Accuracy')\n",
    "#     plt.title('Model Accuracy')\n",
    "#     plt.xlabel('Epochs')\n",
    "#     plt.ylabel('Accuracy')\n",
    "#     plt.legend()\n",
    "\n",
    "#     # Plot Loss Training & Validasi\n",
    "#     plt.subplot(1, 2, 2)\n",
    "#     plt.plot(epochs, history_df['loss'], label='Training Loss')\n",
    "#     plt.plot(epochs, history_df['val_loss'], label='Validation Loss')\n",
    "#     plt.title('Model Loss')\n",
    "#     plt.xlabel('Epochs')\n",
    "#     plt.ylabel('Loss')\n",
    "#     plt.legend()\n",
    "\n",
    "#     plt.show()\n",
    "\n",
    "# # Evaluate the restored model on the test set\n",
    "# loss, accuracy = model.evaluate(X_test, y_test)\n",
    "# print(f\"Test Loss: {loss}, Test Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54bd2800",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, log_loss, accuracy_score, classification_report, confusion_matrix, roc_auc_score, roc_curve, average_precision_score, precision_recall_curve\n",
    "from itertools import cycle\n",
    "\n",
    "# Prediksi menggunakan model pada data uji\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_probabilities = tf.nn.softmax(y_pred).numpy()  # Terapkan softmax secara eksplisit jika perlu\n",
    "y_pred_classes = np.argmax(y_pred_probabilities, axis=1)\n",
    "y_true_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Hitung metrik evaluasi\n",
    "accuracy = accuracy_score(y_true_classes, y_pred_classes)\n",
    "precision = precision_score(y_true_classes, y_pred_classes, average='weighted')\n",
    "recall = recall_score(y_true_classes, y_pred_classes, average='weighted')\n",
    "f1 = f1_score(y_true_classes, y_pred_classes, average='weighted')\n",
    "log_loss_value = log_loss(y_true_classes, y_pred_probabilities)\n",
    "\n",
    "# Print metrik evaluasi\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"Log Loss: {log_loss_value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ff61fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tampilkan classification report yang lengkap\n",
    "report = classification_report(y_true_classes, y_pred_classes, target_names=AKSARA)\n",
    "print(report)\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_true_classes, y_pred_classes)\n",
    "\n",
    "# Normalisasi confusion matrix untuk mendapatkan persentase\n",
    "conf_matrix_normalized = conf_matrix.astype('float') / conf_matrix.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "# Tampilkan confusion matrix sebagai heatmap\n",
    "plt.figure(figsize=(18, 15))\n",
    "sns.heatmap(conf_matrix_normalized, annot=True, fmt=\".2%\", cmap=\"YlGnBu\", xticklabels=AKSARA, yticklabels=AKSARA)\n",
    "plt.xlabel(\"Predicted Aksara\")\n",
    "plt.ylabel(\"Actual Aksara\")\n",
    "plt.title(\"Confusion Matrix (Percentage)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a6cfca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Hitung AUC-ROC untuk setiap kelas\n",
    "# auc_roc_scores = roc_auc_score(y_test, y_pred_probabilities, average=None)\n",
    "# for idx, score in enumerate(auc_roc_scores):\n",
    "#     print(f\"AUC-ROC for class {AKSARA[idx]}: {score:.4f}\")\n",
    "\n",
    "# # Tampilkan ROC Curve untuk setiap kelas\n",
    "# fpr = dict()\n",
    "# tpr = dict()\n",
    "# roc_auc = dict()\n",
    "# n_classes = len(AKSARA)\n",
    "# colors = cycle(['aqua', 'darkorange', 'cornflowerblue', 'green', 'red', 'blue', 'purple', 'pink', 'brown', 'grey', \n",
    "#                 'cyan', 'magenta', 'yellow', 'black', 'lime', 'navy', 'gold', 'teal', 'salmon', 'olive'])\n",
    "\n",
    "# plt.figure(figsize=(10, 8))\n",
    "\n",
    "# for i, color in zip(range(n_classes), colors):\n",
    "#     fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_pred_probabilities[:, i])\n",
    "#     roc_auc[i] = roc_auc_score(y_test[:, i], y_pred_probabilities[:, i])\n",
    "#     plt.plot(fpr[i], tpr[i], color=color, lw=2, label=f'ROC curve of class {AKSARA[i]} (area = {roc_auc[i]:.2f})')\n",
    "\n",
    "# plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "# plt.xlim([0.0, 1.0])\n",
    "# plt.ylim([0.0, 1.05])\n",
    "# plt.xlabel('False Positive Rate')\n",
    "# plt.ylabel('True Positive Rate')\n",
    "# plt.title('Receiver Operating Characteristic (ROC) Curve for each class')\n",
    "# plt.legend(loc=\"lower right\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ecc9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Hitung AUPRC untuk setiap kelas\n",
    "# average_precision = dict()\n",
    "# precision_recall_curves = dict()\n",
    "\n",
    "# plt.figure(figsize=(10, 8))\n",
    "\n",
    "# for i, color in zip(range(n_classes), colors):\n",
    "#     precision_recall_curves[i], recall, _ = precision_recall_curve(y_test[:, i], y_pred_probabilities[:, i])\n",
    "#     average_precision[i] = average_precision_score(y_test[:, i], y_pred_probabilities[:, i])\n",
    "#     plt.plot(recall, precision_recall_curves[i], color=color, lw=2, label=f'PR curve of class {AKSARA[i]} (area = {average_precision[i]:.2f})')\n",
    "\n",
    "# plt.xlabel('Recall')\n",
    "# plt.ylabel('Precision')\n",
    "# plt.title('Precision-Recall Curve for each class')\n",
    "# plt.legend(loc=\"lower right\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00678533",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from keras.models import load_model\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Define the function to display images with predictions\n",
    "# def display_images_with_predictions(images, true_labels, predictions, labels, max_images=20):\n",
    "#     num_images = min(len(images), max_images)\n",
    "#     cols = 5\n",
    "#     rows = (num_images + cols - 1) // cols  # Compute number of rows needed\n",
    "\n",
    "#     plt.figure(figsize=(15, 3 * rows))\n",
    "    \n",
    "#     for i in range(num_images):\n",
    "#         plt.subplot(rows, cols, i + 1)\n",
    "#         plt.imshow(images[i].squeeze(), cmap='gray')\n",
    "        \n",
    "#         # Check if prediction is correct\n",
    "#         correct = true_labels[i] == predictions[i]\n",
    "#         result = \"Correct\" if correct else \"Incorrect\"\n",
    "#         color = \"green\" if correct else \"red\"\n",
    "        \n",
    "#         # Adjust title to avoid overlapping\n",
    "#         plt.title(\n",
    "#             f\"True: {labels[true_labels[i]]}\\nPred: {labels[predictions[i]]}\\n{result}\",\n",
    "#             color=color,\n",
    "#             fontsize=10,  # Set font size\n",
    "#             pad=10  # Add padding to title\n",
    "#         )\n",
    "#         plt.axis('off')\n",
    "    \n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "# # Load the trained model\n",
    "# # model_path = \"C:\\\\Users\\\\USER-03\\\\W\\\\projects_s\\\\save_models\\\\save_models_h5\\\\v4.2.7_model1.h5\"\n",
    "# model = load_model(model_filepath_v1)\n",
    "\n",
    "# # Perform predictions\n",
    "# y_pred_probabilities = model.predict(X_test)\n",
    "# y_pred_classes = np.argmax(y_pred_probabilities, axis=1)\n",
    "\n",
    "# # Convert one-hot encoded y_test to class indices if needed\n",
    "# if y_test.ndim == 2:\n",
    "#     y_test_classes = np.argmax(y_test, axis=1)\n",
    "# else:\n",
    "#     y_test_classes = y_test\n",
    "\n",
    "# # Display some test images with predictions\n",
    "# display_images_with_predictions(X_test, y_test_classes, y_pred_classes, AKSARA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e74678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import cv2\n",
    "# from keras.preprocessing import image\n",
    "# from keras.models import load_model\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Define the custom preprocessing function\n",
    "# def apply_custom_preprocessing(image):\n",
    "#     # Convert image to uint8 if not already\n",
    "#     if image.dtype != np.uint8:\n",
    "#         image = np.uint8(image)\n",
    "    \n",
    "#     # Apply Gaussian Blur\n",
    "#     ApplyGaussian = cv2.GaussianBlur(image, (9, 9), 10.0)\n",
    "    \n",
    "#     # Enhance the image sharpness\n",
    "#     img = cv2.addWeighted(image, 1.5, ApplyGaussian, -0.5, 0)\n",
    "    \n",
    "#     # Apply sharpening filter\n",
    "#     kernel = np.array([[-1, -1, -1], [-1, 9, -1], [-1, -1, -1]])\n",
    "#     img = cv2.filter2D(img, -1, kernel)\n",
    "    \n",
    "#     # Remove noise using median filter\n",
    "#     img = cv2.medianBlur(img, 1)\n",
    "    \n",
    "#     # Apply Otsu's thresholding\n",
    "#     _, img = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    \n",
    "#     # Invert the image (background to black, text to white)\n",
    "#     img = 255 - img\n",
    "    \n",
    "#     return img\n",
    "\n",
    "# # Define the function to preprocess the image\n",
    "# def preprocess_image(img_path, image_size):\n",
    "#     # Load the image\n",
    "#     img = image.load_img(img_path, target_size=image_size, color_mode='grayscale')\n",
    "#     # Convert the image to array\n",
    "#     img_array = image.img_to_array(img)\n",
    "    \n",
    "#     # Apply custom preprocessing\n",
    "#     img_array_preprocessed = apply_custom_preprocessing(img_array)\n",
    "    \n",
    "#     # Normalize the image\n",
    "#     img_array_preprocessed = img_array_preprocessed.astype(\"float32\") / 255.0\n",
    "#     # Expand dimensions to match input shape\n",
    "#     img_array_preprocessed = np.expand_dims(img_array_preprocessed, axis=-1)\n",
    "#     img_array_preprocessed = np.expand_dims(img_array_preprocessed, axis=0)\n",
    "    \n",
    "#     return img_array_preprocessed\n",
    "\n",
    "# # Load the trained model\n",
    "# model = load_model(path_model_save)  # Replace with your model path\n",
    "\n",
    "# # Path to the test image\n",
    "# test_image_path = \"C:\\\\Users\\\\wawn1\\\\projects_skripsi\\\\data\\\\data_original\\\\CustomData\\\\data_prediction\\\\pa10.png\"\n",
    "# image_size = IMAGE_SIZE\n",
    "\n",
    "# # Preprocess the image\n",
    "# preprocessed_image = preprocess_image(test_image_path, image_size)\n",
    "\n",
    "# # Perform the prediction\n",
    "# output = model.predict(preprocessed_image)\n",
    "\n",
    "# # Find the index of the maximum value in the output\n",
    "# pos = np.argmax(output)\n",
    "\n",
    "# # Print the result based on the index\n",
    "# print(\"Prediction output:\\n\", output)\n",
    "# print(\"Score: \", output[0][pos])\n",
    "\n",
    "# # Define a variable to store the predicted label\n",
    "# predicted_label = \"\"\n",
    "\n",
    "# # Print the class based on the position using if-elif\n",
    "# if pos == 0:\n",
    "#     predicted_label = \"ba\"\n",
    "# elif pos == 1:\n",
    "#     predicted_label = 'ca'\n",
    "# elif pos == 2:\n",
    "#     predicted_label = 'da'\n",
    "# elif pos == 3:\n",
    "#     predicted_label = 'dha'\n",
    "# elif pos == 4:\n",
    "#     predicted_label = 'ga'\n",
    "# elif pos == 5:\n",
    "#     predicted_label = 'ha'\n",
    "# elif pos == 6:\n",
    "#     predicted_label = 'ja'\n",
    "# elif pos == 7:\n",
    "#     predicted_label = 'ka'\n",
    "# elif pos == 8:\n",
    "#     predicted_label = 'la'\n",
    "# elif pos == 9:\n",
    "#     predicted_label = 'ma'\n",
    "# elif pos == 10:\n",
    "#     predicted_label = 'na'\n",
    "# elif pos == 11:\n",
    "#     predicted_label = 'nga'\n",
    "# elif pos == 12:\n",
    "#     predicted_label = 'nya'\n",
    "# elif pos == 13:\n",
    "#     predicted_label = 'pa'\n",
    "# elif pos == 14:\n",
    "#     predicted_label = 'ra'\n",
    "# elif pos == 15:\n",
    "#     predicted_label = 'sa'\n",
    "# elif pos == 16:\n",
    "#     predicted_label = 'ta'\n",
    "# elif pos == 17:\n",
    "#     predicted_label = 'tha'\n",
    "# elif pos == 18:\n",
    "#     predicted_label = 'wa'\n",
    "# elif pos == 19:\n",
    "#     predicted_label = 'ya'\n",
    "\n",
    "# # Print the predicted label\n",
    "# print(\"Aksara: \", predicted_label)\n",
    "\n",
    "# # Display the preprocessed image with prediction\n",
    "# plt.figure(figsize=(5, 5))\n",
    "\n",
    "# # Preprocessed Image\n",
    "# preprocessed_for_display = apply_custom_preprocessing(image.img_to_array(image.load_img(test_image_path, target_size=image_size, color_mode='grayscale')))\n",
    "# plt.imshow(preprocessed_for_display.squeeze(), cmap='gray')\n",
    "# plt.title(f\"Predicted: {predicted_label} || Score: {output[0][pos]:.8f}\")\n",
    "# plt.axis('on')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4044ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import cv2\n",
    "# from keras.preprocessing import image\n",
    "# from keras.models import load_model\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Define the custom preprocessing function\n",
    "# def apply_custom_preprocessing(image):\n",
    "#     # Convert image to uint8 if not already\n",
    "#     if image.dtype != np.uint8:\n",
    "#         image = np.uint8(image)\n",
    "    \n",
    "#     # Apply Gaussian Blur\n",
    "#     ApplyGaussian = cv2.GaussianBlur(image, (9, 9), 10.0)\n",
    "    \n",
    "#     # Enhance the image sharpness\n",
    "#     img = cv2.addWeighted(image, 1.5, ApplyGaussian, -0.5, 0)\n",
    "    \n",
    "#     # Apply sharpening filter\n",
    "#     kernel = np.array([[-1, -1, -1], [-1, 9, -1], [-1, -1, -1]])\n",
    "#     img = cv2.filter2D(img, -1, kernel)\n",
    "    \n",
    "#     # Remove noise using median filter\n",
    "#     img = cv2.medianBlur(img, 1)\n",
    "    \n",
    "#     # Apply Otsu's thresholding\n",
    "#     _, img = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    \n",
    "#     # Invert the image (background to black, text to white)\n",
    "#     img = 255 - img\n",
    "    \n",
    "#     return img\n",
    "\n",
    "# # Define the function to preprocess the image\n",
    "# def preprocess_image(img_path, image_size):\n",
    "#     # Load the image\n",
    "#     img = image.load_img(img_path, target_size=image_size, color_mode='grayscale')\n",
    "#     # Convert the image to array\n",
    "#     img_array = image.img_to_array(img)\n",
    "    \n",
    "#     # Apply custom preprocessing\n",
    "#     img_array_preprocessed = apply_custom_preprocessing(img_array)\n",
    "    \n",
    "#     # Normalize the image\n",
    "#     img_array_preprocessed = img_array_preprocessed.astype(\"float32\") / 255.0\n",
    "#     # Expand dimensions to match input shape\n",
    "#     img_array_preprocessed = np.expand_dims(img_array_preprocessed, axis=-1)\n",
    "#     img_array_preprocessed = np.expand_dims(img_array_preprocessed, axis=0)\n",
    "    \n",
    "#     return img_array_preprocessed, img_array\n",
    "\n",
    "# # Load the trained model\n",
    "# model = load_model(path_model_save)  # Replace with your model path\n",
    "\n",
    "# # Path to the test image\n",
    "# test_image_path = \"C:\\\\Users\\\\wawn1\\\\projects_skripsi\\\\data\\\\data_original\\\\CustomData\\\\data_prediction\\\\dha11.png\"\n",
    "# image_size = IMAGE_SIZE\n",
    "\n",
    "# # Preprocess the image\n",
    "# preprocessed_image, original_image = preprocess_image(test_image_path, image_size)\n",
    "\n",
    "# # Perform the prediction\n",
    "# output = model.predict(preprocessed_image)\n",
    "\n",
    "# # Find the index of the maximum value in the output\n",
    "# max_val = output[0][0]\n",
    "# pos = 0\n",
    "# for i in range(1, len(output[0])):\n",
    "#     if output[0][i] > max_val:\n",
    "#         max_val = output[0][i]\n",
    "#         pos = i\n",
    "\n",
    "# # Print the result based on the index\n",
    "# print(\"Prediction output:\\n\", output)\n",
    "# print(\"Score: \", max_val)\n",
    "\n",
    "# # Define labels\n",
    "# labels = ['ba', 'ca', 'da', 'dha', 'ga', 'ha', 'ja', 'ka', 'la', 'ma', \n",
    "#           'na', 'nga', 'nya', 'pa', 'ra', 'sa', 'ta', 'tha', 'wa', 'ya']\n",
    "# predicted_label = labels[pos]\n",
    "# print(\"Aksara: \", predicted_label)\n",
    "\n",
    "# # Display the preprocessed image with prediction\n",
    "# plt.figure(figsize=(5, 5))\n",
    "\n",
    "# # Preprocessed Image\n",
    "# preprocessed_for_display = apply_custom_preprocessing(image.img_to_array(image.load_img(test_image_path, target_size=image_size, color_mode='grayscale')))\n",
    "# plt.imshow(preprocessed_for_display.squeeze(), cmap='gray')\n",
    "# plt.title(f\"Predicted: {predicted_label} || Score: {max_val:.8f}\")\n",
    "# plt.axis('on')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874e6275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Perform the prediction\n",
    "# output = model.predict(preprocessed_image)\n",
    "\n",
    "# # Find the index of the maximum value in the output\n",
    "# pos = np.argmax(output)\n",
    "\n",
    "# # Print the result based on the index\n",
    "# predicted_label = labels[pos]\n",
    "# max_val = output[0][pos]\n",
    "\n",
    "# print(\"Prediction output:\\n\", output)\n",
    "# print(\"Score: \", max_val)\n",
    "# print(\"Aksara: \", predicted_label)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
